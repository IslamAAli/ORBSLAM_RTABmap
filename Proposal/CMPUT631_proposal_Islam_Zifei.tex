\documentclass[12pt]{article}
\usepackage[margin=1in]{geometry}  % set the margins to 1in on all sides
\usepackage{graphicx}              % to include figures
\usepackage{epstopdf}
\usepackage{amsmath}               % great math stuff
\usepackage{amsfonts}              % for blackboard bold, etc
\usepackage{amsthm}                % better theorem environments
\usepackage{amssymb}
\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{fancyhdr}
\usepackage{tikz}
\usepackage{lipsum,array,amsmath}
\usetikzlibrary{positioning,automata}
\usetikzlibrary{arrows.meta}
\usepackage{pgfplots}
\usepackage{amsmath}
\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}
\usepackage{hyperref}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{float}


\pagestyle{fancy}
\fancyhf{}

\lhead{CMPUT-631: Project Proposal}
\rfoot{Page \thepage}
\lfoot{\scriptsize{}}


\begin{document}


% =================== Header ====================
\begin{center}
{ CMPUT 631: Project Proposal}  \\ 
\Large{\textbf{Performance Enhancement of RTAB-map Utilizing ORB-SLAM2 Depth Information}}  \\
\vspace{.1in}
\begin{table}[H]
\center
\begin{tabular}{clc}
\begin{tabular}[c]{@{}c@{}}Islam Ali\\ iaali@ualberta.ca\end{tabular} & \hspace{3cm} & \begin{tabular}[c]{@{}c@{}}Zifei Jiang\\ zifei.jiang@ualberta.ca\end{tabular}
\end{tabular}
\end{table}
\end{center}
\vspace{5pt}
\hrule
\vspace{2pt}
\section*{Abstract}


\vspace{5pt}
\hrule
\vspace{10pt}
\section{Problem Definition}
\section{Background}
\indent \textbf{SLAM (Simultaneous Localization and Mapping)} is the process of generating a model of the environment (a map) and localizing the camera inside it \cite{cadena2016past}. By localization we refer to the estimation of the 6DoF of the moving camera (orientation and position). Many sensors are used in SLAM such as LiDARs, Cameras, and Radars \cite{bresson2017simultaneous}. In this work, we focus on the usage of vision-based SLAM and possibly its integration with Laser-based SLAM.

The conventional pipeline of SLAM consists of two major stages, the first one is responsible for sensor abstraction and processing, which includes features extraction and tracking as well as any long-term data association steps such as bundle adjustment or loop closure \cite{bresson2017simultaneous}\cite{cadena2016past}. The second step is to use the associated data to estimate a map or a model of the environment and, at the same time, estimate the 6DoFs (complete pose) of the camera inside the generated map reliably. 
\section{Literature Review}
\subsection{Visual SLAM}
\indent A wide spectrum of research has been conducted into the SLAM problem with the objective of enhancing its performance, allow for long-term operation, and enable portable and limited resources applications to utilize it in real-time. SLAM algorithms can be categorized to either feature-based method and direct methods \cite{taketomi2017visual}. Feature-based methods depend on extracting features and tracking them through different captured images. The estimation of the camera motion and the map is then determined using either filter-based methods (such as EKF in MonoSLAM) or using bundle adjustment optimization (such as BA used in PTAM). It was shown that BA method can provide better performance due to its ability to use more feature key points when compared to the complex EKF-based solution \cite{strasdat2012visual}. 

On the contrary, direct feature-less methods are usually used on images directly without any abstraction \cite{taketomi2017visual} in order to achieve real-time operation and eliminating the accumulated error usually occurring in the aforementioned feature-based methods. Direct methods differ, mainly, in the density of the map and can be categorized based on that to either dense, semi-dense, or sparse methods \cite{taketomi2017visual}. Methods such as DTAM and LSD-SLAM are examples of the trials done to achieve direct SLAM by means of dense and semi-dense maps while other methods such as SVO and DVO are more advanced steps towards fully direct SLAM algorithms utilizing spare maps \cite{cadena2016past}\cite{taketomi2017visual}.

\subsection{RTAB-Map}
\subsection{ORB-SLAM 1 \& 2}
\subsection{RTAB-Map vs. ORB-SLAM 2}

\section{Research Methodology and Procedure}
\section{Performance Evaluation}
The three corner stones of performance in computer science are accuracy, efficiency, and storage requirements. In this work, we focus on addressing \textit{accuracy} aspects of the system which are defined by the relation between the generated trajectory and ground truth data. 

The \textit{Absolute Trajectory Error (ATE)} was defined in \cite{8710464} as a performance metric to stand on the accuracy of a SLAM system vs. a ground truth. The ATE is defined by:
\begin{equation}
ATE(t_i) = \Vert (x_{t_i}^*,y_{t_i}^*) - (x_{t_i},y_{t_i}) \Vert
\end{equation}
where $(x_{t_i}^*,y_{t_i}^*)$ are the ground truth coordinates at $t_i$ , and $(x_{t_i},y_{t_i})$ are the coordinates generated by the SLAM algorithm under test at the same epoch. For a more detailed representation of this performance metric, statistical functions are applied to this metric such as Root Mean Square (RMS), mean, median, variance, and standard deviation. Additionally, the maximum and minimum $ATE$ are reported to define the system limits while operating. 
\section{Significance of Proposed Research}

\bibliographystyle{ieeetr}
\bibliography{journal09}
 
\end{document}